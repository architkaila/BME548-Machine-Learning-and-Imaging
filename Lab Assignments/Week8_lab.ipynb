{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Pooling, Normalizations, and Custom layers"
      ],
      "metadata": {
        "id": "O_NT6_9MwLiP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resources this week\n",
        "\n",
        "1. [Ian Goodfellow on BatchNorm](https://www.youtube.com/watch?v=Xogn6veSyxA&t=325s)\n",
        "2. Slide decks [5](http://cs231n.stanford.edu/slides/2022/lecture_5_ruohan.pdf), [6](http://cs231n.stanford.edu/slides/2022/lecture_6_jiajun.pdf) and [7](http://cs231n.stanford.edu/slides/2022/lecture_7_ruohan.pdf) from Stanford CS231n\n",
        "3. [Practical Recommendations for Gradient-Based Training of Deep\n",
        "Architectures](https://arxiv.org/pdf/1206.5533v2.pdf) by Yoshua Bengio\n",
        "\n",
        "The slide decks from Stanford are incredibly detailed and they *will* get you up to speed on many, many things about CNNs as well as provide practical recommendations on how to effectively train a network. We would't be able to do a better job at explaining these details so we are simply going to provide the links. It is highly recommended that you check them out and go through them at some point."
      ],
      "metadata": {
        "id": "XUAOH_CrGST7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Zc0ZHHd9wJg5"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pooling\n",
        "\n",
        "Intuitively, the exact location of a feature is less important than its rough location relative to other features. This is the idea behind the use of pooling. The pooling layer serves to progressively reduce the spatial size of the representation, to reduce the number of parameters, memory footprint and amount of computation in the network, and hence to also control overfitting.\n",
        "\n",
        "TLDR; Pooling is a form of non-linear downsampling to reduce memory usage and add local translation invariance.\n",
        "\n",
        "There are two main types pooling: average pooling and max pooling. They partition the input image into a set of rectangles and, for each such sub-region, average pooling ouputs one value that is the mean of all the values in the sub-region and max pooling outputs the highest value in the subregion."
      ],
      "metadata": {
        "id": "YJ96uR-GwV6D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![maxpool](https://www.researchgate.net/publication/340812216/figure/fig4/AS:928590380138496@1598404607456/Pooling-layer-operation-oproaches-1-Pooling-layers-For-the-function-of-decreasing-the.png)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5fDpy9LT11xT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a tensor with small ints and convert to float\n",
        "A_np = np.random.randint(0, 9, size=(4,4))\n",
        "A = tf.constant(A_np, tf.float32)\n",
        "print(A_np)\n",
        "# apply 2x2 max pooling\n",
        "maxpooled = tf.keras.layers.MaxPooling2D((2,2))(A) # will get an error, why?"
      ],
      "metadata": {
        "id": "h-trDwL01Eq8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "outputId": "5a797cf3-f146-48c8-8550-97e55aabf135"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[4 5 2 5]\n",
            " [0 1 3 8]\n",
            " [6 8 8 4]\n",
            " [0 6 7 3]]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-e5224a2d23db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# apply 2x2 max pooling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmaxpooled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# will get an error, why?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    233\u001b[0m                     \u001b[0;34mf'Input {input_index} of layer \"{layer_name}\" '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m                     \u001b[0;34m\"is incompatible with the layer: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"max_pooling2d\" is incompatible with the layer: expected ndim=4, found ndim=2. Full shape received: (4, 4)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A = tf.reshape(A, (1,4,4,1))\n",
        "maxpooled = tf.keras.layers.MaxPooling2D((2,2))(A)\n",
        "tf.reshape(maxpooled, (2,2)) # for better printing"
      ],
      "metadata": {
        "id": "GOVd9Hai6CFo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b374347a-1061-4f52-e0bf-e9cabb6574f2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
              "array([[5., 8.],\n",
              "       [8., 8.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "avgpooled = tf.keras.layers.AveragePooling2D((2,2))(A)\n",
        "tf.reshape(avgpooled, (2,2)) # for better printing"
      ],
      "metadata": {
        "id": "pCMgp-4O7S6s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8043065-1a34-4ede-ade9-7b33989a6686"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
              "array([[2.5, 4.5],\n",
              "       [5. , 5.5]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try to simulate what happens if my activations shift a pixel to the right. We will shift all values to left and add zero padding."
      ],
      "metadata": {
        "id": "JeMZAFTh76RN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pixel_shift = np.concatenate((np.zeros((4,1)), A_np[:,:3]), axis=1)\n",
        "print(pixel_shift)\n",
        "shifted = tf.constant(pixel_shift, tf.float32)\n",
        "shifted = tf.reshape(shifted, (1,4,4,1))\n",
        "maxpooled = tf.keras.layers.MaxPooling2D((2,2))(shifted)\n",
        "tf.reshape(maxpooled, (2,2)) # for better printing\n",
        "# tf.reshape(pixel_shift, (4,4)) # for better printing"
      ],
      "metadata": {
        "id": "897nKNES74CX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b941fd4f-6f17-462b-a384-55dcc0c689c1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 4. 5. 2.]\n",
            " [0. 0. 1. 3.]\n",
            " [0. 6. 8. 8.]\n",
            " [0. 0. 6. 7.]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
              "array([[4., 5.],\n",
              "       [6., 8.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "My guess is that it will be very similar to the non-shifted version. Over a large image, the changes are very minor. Additionally, in a large network we typically have many maxpooling layers. So by using maxpooling, we can account for small shifts. This is called local translational invariance."
      ],
      "metadata": {
        "id": "-lTgEQJMB6O3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⏰Exercise\n",
        "Will average pooling also result in local translation invariance? Briefly tell us or show us why you think so.\n",
        "\n",
        "I think yes, just like Max Pooling, Avg Pooling will also result in local translation invariance. The Avg Pooling replaces a local patch of the feature map with avg value in that local region. The output from pooling layer is very similar to the input with some minor changes (the values will be replaced by avg values in that region)"
      ],
      "metadata": {
        "id": "qARgPoO1CYdk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batch Normalization\n",
        "If the input activations are too large or are scaled differently (for example if one takes values between (100, 200) and other between (1,2)), that could lead to a very diffcult optimization. So, what if we just forced all the activations to behave \"nicely\"? We can do that using Batch Normalization.\n",
        "I am not going to write the full mathematical details for Batch Norm. I highly recommend checking out the provided resources to learn more. This will help you understand when it can be helpful."
      ],
      "metadata": {
        "id": "h8rmP3w8CpTL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batch Norms (or any other norm layers) are tricky and there is not always an agreement on what is the most useful way to apply them. They also don't necessarily work well with some activations or architectures. Although you might find some literature on whether you should be using BatchNorm or not for your use case, many times, trial and error is your best friend. The order of ops I normally follow is:\n",
        "\n",
        "-> CONV/FC -> BatchNorm -> ReLu(or other activation) -> Dropout -> CONV/FC ->"
      ],
      "metadata": {
        "id": "M5ux860OGsto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# in tensorflow, we can use batchnorm as follows. Please look at the documentation\n",
        "# for usage instructions.\n",
        "tf.keras.layers.BatchNormalization()"
      ],
      "metadata": {
        "id": "JCTBwwTbF3sh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53a8664b-d2b2-463f-b98c-1b5953cc16c9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.layers.normalization.batch_normalization.BatchNormalization at 0x7fa01bcf7580>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Layers\n",
        "\n",
        "Most of the time when writing code for machine learning models you want to operate at a higher level of abstraction than individual operations and manipulation of individual variables.\n",
        "\n",
        "Many machine learning models are expressible as the composition and stacking of relatively simple layers, and TensorFlow provides both a set of many common layers as well as easy ways for you to write your own application-specific layers either from scratch or as the composition of existing layers."
      ],
      "metadata": {
        "id": "KsoKQynzH0k5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Layer class is one of the fundamental abstractions in TensorFlow provided via the `tf.keras.Layer` class. A Layer encapsulates a state (weights) and some computation (defined in the call method). A `tf.keras.Layer` has three important things in it:\n",
        "1. `__init__` , where you can do all input-independent initialization\n",
        "2. `build`, where you know the shapes of the input tensors and can do the rest of the initialization\n",
        "3. `call`, where you do the forward pass computation\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3doTJkIrvWzf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0kDbE54-5VS"
      },
      "source": [
        "The best way to implement your own layer is extending the `tf.keras.Layer` class and implementing aforementioned three things.\n",
        "\n",
        "Note that you don't have to wait until `build` is called to create your variables, you can also create them in `__init__`. However, the advantage of creating them in `build` is that it enables late variable creation based on the shape of the inputs the layer will operate on. On the other hand, creating variables in `__init__` would mean that shapes required to create the variables will need to be explicitly specified."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A simple layer (without the build method) would look like:"
      ],
      "metadata": {
        "id": "ryZDLlwTwk8E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YcS3UEyWt5RD"
      },
      "outputs": [],
      "source": [
        "class LinearWithoutBuild(tf.keras.layers.Layer):\n",
        "    \"\"\"y = w.x + b\"\"\"\n",
        "\n",
        "    def __init__(self, units=32, input_dim=32):\n",
        "        super().__init__()\n",
        "        w_init = tf.random_normal_initializer()\n",
        "        self.w = tf.Variable(\n",
        "            initial_value=w_init(shape=(input_dim, units), dtype=\"float32\"),\n",
        "            trainable=True,\n",
        "        )\n",
        "        b_init = tf.zeros_initializer()\n",
        "        self.b = tf.Variable(\n",
        "            initial_value=b_init(shape=(units,), dtype=\"float32\"), trainable=True\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.matmul(inputs, self.w) + self.b\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FYI, units are dimensionality of the output space.\n",
        "\n",
        "Although the better way is to do:"
      ],
      "metadata": {
        "id": "7KesHhQOw3sr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear(tf.keras.layers.Layer):\n",
        "    \"\"\"y = w.x + b\"\"\"\n",
        "\n",
        "    def __init__(self, units=32):\n",
        "        super().__init__()\n",
        "        self.units = units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.w = self.add_weight(\n",
        "            shape=(input_shape[-1], self.units),\n",
        "            initializer=\"random_normal\",\n",
        "            trainable=True,\n",
        "        )\n",
        "        self.b = self.add_weight(\n",
        "            shape=(self.units,), initializer=\"random_normal\", trainable=True\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.matmul(inputs, self.w) + self.b"
      ],
      "metadata": {
        "id": "fU0A8vSHw-DL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This way we don't have to always specify the input dimensions, only output dimensions are needed. This is very useful when we building bigger models with our layers in them. We will not have to calculate input and weight dimensions for every layer, only output dimensions will suffice.\n",
        "\n",
        "You can use this layer like any other layer:"
      ],
      "metadata": {
        "id": "UnJ0wzOYxBfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate our layer.\n",
        "linear_layer = Linear(4)\n",
        "\n",
        "# This will also call `build(input_shape)` and create the weights.\n",
        "y = linear_layer(tf.ones((2, 2))) # first 2 is batch size\n",
        "y"
      ],
      "metadata": {
        "id": "CZS-hlSQxV9w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da9e906e-7c7d-4995-bd21-fa836363cbf3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
              "array([[-0.16900986,  0.01822236,  0.06824723, -0.05567059],\n",
              "       [-0.16900986,  0.01822236,  0.06824723, -0.05567059]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And if you change the input dims from 2 to 5:"
      ],
      "metadata": {
        "id": "6Hm8uHAPx2kH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate our layer.\n",
        "linear_layer = Linear(4)\n",
        "\n",
        "# This will also call `build(input_shape)` and create the weights.\n",
        "y = linear_layer(tf.ones((2, 5))) # first 2 is batch size\n",
        "y"
      ],
      "metadata": {
        "id": "iZF2uX3Jxx89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "410d1a80-c7e0-4082-97dd-234071e89d0e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
              "array([[-0.304095  , -0.10250034, -0.07576159, -0.00484004],\n",
              "       [-0.304095  , -0.10250034, -0.07576159, -0.00484004]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please note that this layer is only an example. Your shapes, initializations and the operations will depend on what precisely you want to implement."
      ],
      "metadata": {
        "id": "xpyQVvC48eLI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⏰ Exercise\n",
        "Copy the layer definition without the `build` method here. Initialize the layer to have an output size of 4. Try to run your layer with different input dimensions like we did above without getting errors. What was different this time?"
      ],
      "metadata": {
        "id": "un80Ozx9yGrW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy layer defination without build method\n",
        "class LinearWithoutBuild(tf.keras.layers.Layer):\n",
        "    \"\"\"y = w.x + b\"\"\"\n",
        "\n",
        "    def __init__(self, units=32, input_dim=32):\n",
        "        super().__init__()\n",
        "        w_init = tf.random_normal_initializer()\n",
        "        self.w = tf.Variable(\n",
        "            initial_value=w_init(shape=(input_dim, units), dtype=\"float32\"),\n",
        "            trainable=True,\n",
        "        )\n",
        "        b_init = tf.zeros_initializer()\n",
        "        self.b = tf.Variable(\n",
        "            initial_value=b_init(shape=(units,), dtype=\"float32\"), trainable=True\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.matmul(inputs, self.w) + self.b\n",
        "\n",
        "# Instantiate our layer with output size 4 and input size 2\n",
        "linear_layer = LinearWithoutBuild(4, 2)\n",
        "# Run layer for input dimen 2\n",
        "y = linear_layer(tf.ones((2, 2))) # first 2 is batch size\n",
        "print(y)\n",
        "\n",
        "# Instantiate our layer with output size 4 and input size 5\n",
        "linear_layer = LinearWithoutBuild(4, 5)\n",
        "# Run layer for input dimen 5\n",
        "y = linear_layer(tf.ones((2, 5))) # first 2 is batch size\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgahnbz0VtIO",
        "outputId": "a6739e34-6a54-4e17-96cb-b9b1d1d402ed"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[-0.09662074 -0.00716301 -0.11841995 -0.10793495]\n",
            " [-0.09662074 -0.00716301 -0.11841995 -0.10793495]], shape=(2, 4), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[ 0.05496375 -0.03915724  0.00444828  0.06658031]\n",
            " [ 0.05496375 -0.03915724  0.00444828  0.06658031]], shape=(2, 4), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bv0iaitMt5RG"
      },
      "source": [
        "## Trainable and non-trainable weights\n",
        "\n",
        "Weights created by layers can be either trainable or non-trainable. They're\n",
        "exposed in `trainable_weights` and `non_trainable_weights` respectively.\n",
        "Here's a layer with a non-trainable weight:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Xbhgu2SLt5RG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d11d5f64-d1f3-44a9-a5a2-821320b6d503"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2. 2.]\n",
            "[4. 4.]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class ComputeSum(tf.keras.layers.Layer):\n",
        "    \"\"\"Returns the sum of the inputs.\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        # Create a non-trainable weight.\n",
        "        self.total = tf.Variable(initial_value=tf.zeros((input_dim,)), trainable=False)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        self.total.assign_add(tf.reduce_sum(inputs, axis=0))\n",
        "        return self.total\n",
        "\n",
        "\n",
        "my_sum = ComputeSum(2)\n",
        "x = tf.ones((2, 2))\n",
        "\n",
        "y = my_sum(x)\n",
        "print(y.numpy())  # [2. 2.]\n",
        "\n",
        "y = my_sum(x)\n",
        "print(y.numpy())  # [4. 4.]\n",
        "\n",
        "assert my_sum.weights == [my_sum.total]\n",
        "assert my_sum.non_trainable_weights == [my_sum.total]\n",
        "assert my_sum.trainable_weights == []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lm7f8aL-t5RH"
      },
      "source": [
        "## Layers that own layers\n",
        "\n",
        "Layers can be recursively nested to create bigger computation blocks.\n",
        "Each layer will track the weights of its sublayers\n",
        "(both trainable and non-trainable)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "u4rcRy8zt5RH"
      },
      "outputs": [],
      "source": [
        "# Let's reuse the Linear class\n",
        "# with a `build` method that we defined above.\n",
        "\n",
        "\n",
        "class MLP(tf.keras.layers.Layer):\n",
        "    \"\"\"Simple stack of Linear layers.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear_1 = Linear(32)\n",
        "        self.linear_2 = Linear(32)\n",
        "        self.linear_3 = Linear(10)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.linear_1(inputs)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.linear_2(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        return self.linear_3(x)\n",
        "\n",
        "\n",
        "mlp = MLP()\n",
        "\n",
        "# The first call to the `mlp` object will create the weights.\n",
        "y = mlp(tf.ones(shape=(3, 64)))\n",
        "\n",
        "# Weights are recursively tracked.\n",
        "assert len(mlp.weights) == 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKiu7Kyft5RI"
      },
      "source": [
        "Note that our manually-created MLP above is equivalent to the following\n",
        "built-in option:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "sPGK7VQNt5RI"
      },
      "outputs": [],
      "source": [
        "mlp = tf.keras.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.Dense(32, activation=tf.nn.relu),\n",
        "        tf.keras.layers.Dense(32, activation=tf.nn.relu),\n",
        "        tf.keras.layers.Dense(10),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQz-xS7Bt5RI"
      },
      "source": [
        "## Optional: Tracking losses created by layers\n",
        "\n",
        "Layers can create losses during the forward pass via the `add_loss()` method.\n",
        "This is especially useful for regularization losses.\n",
        "The losses created by sublayers are recursively tracked by the parent layers.\n",
        "\n",
        "Here's a layer that creates an activity regularization loss:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "vYaV7ed2t5RJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "class ActivityRegularization(tf.keras.layers.Layer):\n",
        "    \"\"\"Layer that creates an activity sparsity regularization loss.\"\"\"\n",
        "\n",
        "    def __init__(self, rate=1e-2):\n",
        "        super().__init__()\n",
        "        self.rate = rate\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # We use `add_loss` to create a regularization loss\n",
        "        # that depends on the inputs.\n",
        "        self.add_loss(self.rate * tf.reduce_sum(inputs))\n",
        "        return inputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "086yvrgwt5RJ"
      },
      "source": [
        "Any model incorporating this layer will track this regularization loss:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "laMOsUQDt5RJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f79db5ff-5d5d-4d06-9d75-347bf361000a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<tf.Tensor: shape=(), dtype=float32, numpy=0.18743014>]\n"
          ]
        }
      ],
      "source": [
        "# Let's use the loss layer in a MLP block.\n",
        "class SparseMLP(tf.keras.layers.Layer):\n",
        "    \"\"\"Stack of Linear layers with a sparsity regularization loss.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear_1 = Linear(32)\n",
        "        self.regularization = ActivityRegularization(1e-2)\n",
        "        self.linear_3 = Linear(10)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.linear_1(inputs)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.regularization(x)\n",
        "        return self.linear_3(x)\n",
        "\n",
        "\n",
        "mlp = SparseMLP()\n",
        "y = mlp(tf.ones((10, 10)))\n",
        "\n",
        "print(mlp.losses)  # List containing one float32 scalar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfJukXUkt5RK"
      },
      "source": [
        "These losses are cleared by the top-level layer at the start of each forward\n",
        "pass -- they don't accumulate. `layer.losses` always contains only the losses\n",
        "created during the last forward pass. You would typically use these losses by\n",
        "summing them before computing your gradients when writing a training loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "A-l5yAkDt5RK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94b23255-ab92-4fe2-b4aa-66739fa301e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Step: 0 Loss: 5.563202381134033\n",
            "Step: 100 Loss: 2.5958197116851807\n",
            "Step: 200 Loss: 2.415818929672241\n",
            "Step: 300 Loss: 2.3787243366241455\n",
            "Step: 400 Loss: 2.344101905822754\n",
            "Step: 500 Loss: 2.355374813079834\n",
            "Step: 600 Loss: 2.331726551055908\n",
            "Step: 700 Loss: 2.3417627811431885\n",
            "Step: 800 Loss: 2.343681812286377\n",
            "Step: 900 Loss: 2.314337968826294\n"
          ]
        }
      ],
      "source": [
        "# Losses correspond to the *last* forward pass.\n",
        "mlp = SparseMLP()\n",
        "mlp(tf.ones((10, 10)))\n",
        "assert len(mlp.losses) == 1\n",
        "mlp(tf.ones((10, 10)))\n",
        "assert len(mlp.losses) == 1  # No accumulation.\n",
        "\n",
        "# Let's demonstrate how to use these losses in a training loop.\n",
        "\n",
        "# Prepare a dataset.\n",
        "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (x_train.reshape(60000, 784).astype(\"float32\") / 255, y_train)\n",
        ")\n",
        "dataset = dataset.shuffle(buffer_size=1024).batch(64)\n",
        "\n",
        "# A new MLP.\n",
        "mlp = SparseMLP()\n",
        "\n",
        "# Loss and optimizer.\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
        "\n",
        "for step, (x, y) in enumerate(dataset):\n",
        "    with tf.GradientTape() as tape:\n",
        "\n",
        "        # Forward pass.\n",
        "        logits = mlp(x)\n",
        "\n",
        "        # External loss value for this batch.\n",
        "        loss = loss_fn(y, logits)\n",
        "\n",
        "        # Add the losses created during the forward pass.\n",
        "        loss += sum(mlp.losses)\n",
        "\n",
        "        # Get gradients of the loss wrt the weights.\n",
        "        gradients = tape.gradient(loss, mlp.trainable_weights)\n",
        "\n",
        "    # Update the weights of our linear layer.\n",
        "    optimizer.apply_gradients(zip(gradients, mlp.trainable_weights))\n",
        "\n",
        "    # Logging.\n",
        "    if step % 100 == 0:\n",
        "        print(\"Step:\", step, \"Loss:\", float(loss))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⏰ Exercise\n",
        "Make and use a Resnet block using custom layer creation.\n",
        "A Resnet block looks like this:\n",
        "\n",
        "![resnet block](https://d2l.ai/_images/resnet-block.svg)\n",
        "\n",
        "What is the Resnet?\n",
        "\n",
        "[Paper](https://arxiv.org/pdf/1512.03385.pdf)\n",
        "\n",
        "[Resources](https://towardsdatascience.com/understanding-and-visualizing-resnets-442284831be8)\n",
        "\n",
        "\n",
        "You can choose to do either of the two versions. Feel free to use the simple predefined layer functions available to you. Have number of channels in the output as a user-specified quantity. Show that your layer works by passing in an image and verifying output shape. For an input of shape $(B,H,W,C_{in})$ the expected output is of shape $(B,H,W,C_{out})$.\n"
      ],
      "metadata": {
        "id": "SmKpS0T1zxns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Resnet_block(tf.keras.layers.Layer):\n",
        "    \"\"\"Simple stack of Linear layers.\"\"\"\n",
        "\n",
        "    def __init__(self, output_chanels):\n",
        "        super().__init__()\n",
        "        self.output_chanels = output_chanels\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        self.conv_1 = tf.keras.layers.Conv2D(filters=self.output_chanels, kernel_size=3, activation=None, padding='same', input_shape=(input_shape[1],input_shape[2],input_shape[3]))\n",
        "        self.batch_norm_layer_1 = tf.keras.layers.BatchNormalization()\n",
        "        self.conv_2 = tf.keras.layers.Conv2D(filters=self.output_chanels, kernel_size=3, activation=None, padding='same')\n",
        "        self.batch_norm_layer_2 = tf.keras.layers.BatchNormalization()\n",
        "        self.downsample_conv_layer = tf.keras.layers.Conv2D(filters=self.output_chanels, kernel_size=1, activation=None)\n",
        "\n",
        "\n",
        "    def call(self, inputs):\n",
        "        ## Resnet Block\n",
        "        x = self.conv_1(inputs)\n",
        "        x = self.batch_norm_layer_1(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.conv_2(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.batch_norm_layer_2(x)\n",
        "\n",
        "        ## Add the results\n",
        "        x += self.downsample_conv_layer(inputs)\n",
        "        \n",
        "        ## Final activation func\n",
        "        x = tf.nn.relu(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "9L4ZoY0Fx_aO"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_batch = tf.ones(shape=(3, 28, 28, 1))\n",
        "image_batch.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtqIpdS5e5T7",
        "outputId": "270447de-2975-4fe3-8aa9-5296fbdaf7dc"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([3, 28, 28, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Resnet Block\n",
        "resnet = Resnet_block(output_chanels = 8)\n",
        "y = resnet(image_batch)\n",
        "\n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcCF477tg19m",
        "outputId": "304b77a4-2262-4bf2-ccb0-e776daa31345"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 28, 28, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tF_ilS8Fg9TK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}